{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75e23a5-d071-4669-9060-650d0c42e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "# CSC396 - Intro to Deep Learning w/ NLP\n",
    "# Jose Santiago Campa Morales\n",
    "# November 23, 2025\n",
    "# Assignment #4 -- Transformer\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5edb18-9876-4d4b-bece-a6770dce17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3980290 sentences.\n",
      "Using 1000000 sentences for testing\n",
      "Using 1000000 sentences for testing\n"
     ]
    }
   ],
   "source": [
    "# read .txt file\n",
    "sentences = []\n",
    "with open(\"assignment4-dataset.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            sentences.append(line)\n",
    "\n",
    "print(\"Loaded\", len(sentences), \"sentences.\")\n",
    "\n",
    "# Sample dataset for testing\n",
    "SAMPLE_SIZE = 1000000  # Use 1M sentences instead of 4M\n",
    "sentences = sentences[:SAMPLE_SIZE]\n",
    "print(f\"Using {len(sentences)} sentences for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba8a5db-13c1-4a8d-a977-100886b1a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decided to use distilbert\n",
    "\n",
    "# From official website:\n",
    "# Load model directly\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a3ee4c-fa22-4002-acb8-ec67e637d943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3167c1b-560a-42c7-93b4-70cb6f805059",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = tokenizer.pad_token_id\n",
    "id_to_tok = tokenizer.convert_ids_to_tokens(list(range(tokenizer.vocab_size)))\n",
    "\n",
    "token_sums = {}\n",
    "token_counts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea01dde-656c-4d16-a8b5-afc81b21e9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e863ab373d904a63b219fbbed699abff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding:   0%|          | 0/7813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128  # Increased for better GPU utilization\n",
    "max_len = 64\n",
    "\n",
    "for i in tqdm(range(0, len(sentences), batch_size), desc=\"Embedding\"):\n",
    "    batch = sentences[i:i+batch_size]\n",
    "\n",
    "    enc = tokenizer(\n",
    "        batch,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_len\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = model(**enc).last_hidden_state  # [B, T, D]\n",
    "\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    B, T, D = hidden.shape\n",
    "\n",
    "    # Flatten batch\n",
    "    flat_ids = input_ids.reshape(-1)\n",
    "    flat_vecs = hidden.reshape(-1, D)\n",
    "\n",
    "    # Mask padding\n",
    "    mask = flat_ids != pad_id\n",
    "    flat_ids = flat_ids[mask]\n",
    "    flat_vecs = flat_vecs[mask]\n",
    "\n",
    "    # -----------------------\n",
    "    # VECTORIZED AGGREGATION\n",
    "    # -----------------------\n",
    "    unique_ids, inverse = torch.unique(flat_ids, return_inverse=True)\n",
    "    sums = torch.zeros((len(unique_ids), D), device=flat_vecs.device)\n",
    "    sums.index_add_(0, inverse, flat_vecs)\n",
    "    counts = torch.bincount(inverse)\n",
    "\n",
    "    # Move sums and counts to CPU and update global dict\n",
    "    unique_ids = unique_ids.cpu()\n",
    "    sums = sums.cpu()\n",
    "    counts = counts.cpu()\n",
    "\n",
    "    for tok_id, vec_sum, count in zip(unique_ids.tolist(), sums, counts.tolist()):\n",
    "        if tok_id in token_sums:\n",
    "            token_sums[tok_id] += vec_sum\n",
    "            token_counts[tok_id] += count\n",
    "        else:\n",
    "            token_sums[tok_id] = vec_sum.clone()\n",
    "            token_counts[tok_id] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743f9314-b3ec-441e-9ea9-55d922029172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 28591\n"
     ]
    }
   ],
   "source": [
    "token_static_embeddings = {}\n",
    "for tok_id, vec_sum in token_sums.items():\n",
    "    avg_vec = vec_sum / token_counts[tok_id]\n",
    "    tok_str = id_to_tok[tok_id]\n",
    "    token_static_embeddings[tok_str] = avg_vec.numpy()\n",
    "\n",
    "print(\"Number of tokens:\", len(token_static_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2518311d-8ca4-4a9d-a41b-6a617f540fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to static_embeddings_roberta.npy\n"
     ]
    }
   ],
   "source": [
    "np.save(\"static_embeddings_roberta.npy\", token_static_embeddings)\n",
    "print(\"Saved to static_embeddings_roberta.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838bb2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'the'\n",
      "  Embedding shape: (768,)\n",
      "  First 10 dimensions: [-0.5182941  -0.2055869  -0.23243168 -0.04096534  0.138746   -0.04299089\n",
      "  0.14637561  0.48362467 -0.13441496  0.02707357]\n",
      "  Count (how many times seen): 767752\n",
      "\n",
      "Token: 'hello'\n",
      "  Embedding shape: (768,)\n",
      "  First 10 dimensions: [-0.33900633  0.05078349  0.5843242  -0.2721656   0.17722043 -0.2888496\n",
      "  0.21743204  0.2900156  -0.5144144  -0.25633046]\n",
      "  Count (how many times seen): 86\n",
      "\n",
      "Token: 'world'\n",
      "  Embedding shape: (768,)\n",
      "  First 10 dimensions: [ 0.10564563 -0.01599088  0.22037983 -0.2593901   0.29943195 -0.24501027\n",
      "  0.5150999   0.6359968  -0.3580607   0.06160359]\n",
      "  Count (how many times seen): 12616\n",
      "\n",
      "Token: 'computer'\n",
      "  Embedding shape: (768,)\n",
      "  First 10 dimensions: [-0.5075014   0.40926838  0.06141642  0.08055346  1.0685854  -0.23621342\n",
      " -0.45720547  0.42538086 -0.2385968   0.04511508]\n",
      "  Count (how many times seen): 1153\n",
      "\n",
      "Token: 'science'\n",
      "  Embedding shape: (768,)\n",
      "  First 10 dimensions: [-0.3540344   0.4305366  -0.25918427 -0.01950569  0.57324415 -0.07608797\n",
      " -0.08707576  0.2807399  -0.2534341  -0.08924318]\n",
      "  Count (how many times seen): 2767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect a few token embeddings\n",
    "sample_tokens = ['the', 'hello', 'world', 'computer', 'science']\n",
    "\n",
    "for token in sample_tokens:\n",
    "    if token in token_static_embeddings:\n",
    "        embedding = token_static_embeddings[token]\n",
    "        print(f\"Token: '{token}'\")\n",
    "        print(f\"  Embedding shape: {embedding.shape}\")\n",
    "        print(f\"  First 10 dimensions: {embedding[:10]}\")\n",
    "        print(f\"  Count (how many times seen): {token_counts[tokenizer.convert_tokens_to_ids(token)]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4fc880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fname = \"glove.6B.300d-vocabulary.txt\"\n",
    "glove = KeyedVectors.load_word2vec_format(fname, no_header=True)\n",
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b59018c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words from GloVe vocabulary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4fea93ca0b4768823adbe4eb685715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing word embeddings:   0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 399890 words\n",
      "Missing 110 words (tokens not in vocabulary)\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe vocabulary (just words, one per line)\n",
    "with open(\"glove.6B.300d-vocabulary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    glove_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(glove_words)} words from GloVe vocabulary\")\n",
    "\n",
    "# Compute word embeddings by averaging token embeddings\n",
    "word_embeddings = {}\n",
    "missing_words = []\n",
    "\n",
    "for word in tqdm(glove_words, desc=\"Computing word embeddings\"):\n",
    "    # Tokenize the word\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    \n",
    "    # Get embeddings for each token\n",
    "    token_vecs = []\n",
    "    for token in tokens:\n",
    "        if token in token_static_embeddings:\n",
    "            token_vecs.append(token_static_embeddings[token])\n",
    "    \n",
    "    # Average the token embeddings to get word embedding\n",
    "    if len(token_vecs) > 0:\n",
    "        word_embedding = np.mean(token_vecs, axis=0)\n",
    "        word_embeddings[word] = word_embedding\n",
    "    else:\n",
    "        missing_words.append(word)\n",
    "\n",
    "print(f\"Created embeddings for {len(word_embeddings)} words\")\n",
    "print(f\"Missing {len(missing_words)} words (tokens not in vocabulary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f9331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar to 'cactus':\n",
      "  cactuses: 0.8830\n",
      "  coneflower: 0.7930\n",
      "  spineflower: 0.7802\n",
      "  flowerheads: 0.7751\n",
      "  monkeyflower: 0.7714\n",
      "  coneflowers: 0.7633\n",
      "  lemongrass: 0.7622\n",
      "  flowerbed: 0.7604\n",
      "  flowery: 0.7591\n",
      "  crabgrass: 0.7590\n",
      "\n",
      "Most similar to 'cake':\n",
      "  cactuses: 0.8830\n",
      "  coneflower: 0.7930\n",
      "  spineflower: 0.7802\n",
      "  flowerheads: 0.7751\n",
      "  monkeyflower: 0.7714\n",
      "  coneflowers: 0.7633\n",
      "  lemongrass: 0.7622\n",
      "  flowerbed: 0.7604\n",
      "  flowery: 0.7591\n",
      "  crabgrass: 0.7590\n",
      "\n",
      "Most similar to 'cake':\n",
      "  cakebread: 0.9212\n",
      "  cakey: 0.9050\n",
      "  cakelike: 0.9020\n",
      "  caked: 0.8939\n",
      "  cakes: 0.8891\n",
      "  cakewalk: 0.8803\n",
      "  cheesecake: 0.8345\n",
      "  pastry: 0.8221\n",
      "  cheesecakes: 0.8093\n",
      "  shortcake: 0.8080\n",
      "\n",
      "Most similar to 'angry':\n",
      "  cakebread: 0.9212\n",
      "  cakey: 0.9050\n",
      "  cakelike: 0.9020\n",
      "  caked: 0.8939\n",
      "  cakes: 0.8891\n",
      "  cakewalk: 0.8803\n",
      "  cheesecake: 0.8345\n",
      "  pastry: 0.8221\n",
      "  cheesecakes: 0.8093\n",
      "  shortcake: 0.8080\n",
      "\n",
      "Most similar to 'angry':\n",
      "  furious: 0.9081\n",
      "  enraged: 0.9028\n",
      "  anger: 0.9015\n",
      "  angered: 0.8845\n",
      "  angering: 0.8762\n",
      "  annoyed: 0.8720\n",
      "  angerer: 0.8620\n",
      "  outraged: 0.8566\n",
      "  angrily: 0.8533\n",
      "  angers: 0.8521\n",
      "\n",
      "Most similar to 'quickly':\n",
      "  furious: 0.9081\n",
      "  enraged: 0.9028\n",
      "  anger: 0.9015\n",
      "  angered: 0.8845\n",
      "  angering: 0.8762\n",
      "  annoyed: 0.8720\n",
      "  angerer: 0.8620\n",
      "  outraged: 0.8566\n",
      "  angrily: 0.8533\n",
      "  angers: 0.8521\n",
      "\n",
      "Most similar to 'quickly':\n",
      "  swiftly: 0.9187\n",
      "  rapidly: 0.9102\n",
      "  soon: 0.9023\n",
      "  immediately: 0.9021\n",
      "  promptly: 0.8900\n",
      "  instantly: 0.8853\n",
      "  soong: 0.8596\n",
      "  soonest: 0.8501\n",
      "  quickened: 0.8476\n",
      "  rapidly-growing: 0.8435\n",
      "\n",
      "Most similar to 'between':\n",
      "  swiftly: 0.9187\n",
      "  rapidly: 0.9102\n",
      "  soon: 0.9023\n",
      "  immediately: 0.9021\n",
      "  promptly: 0.8900\n",
      "  instantly: 0.8853\n",
      "  soong: 0.8596\n",
      "  soonest: 0.8501\n",
      "  quickened: 0.8476\n",
      "  rapidly-growing: 0.8435\n",
      "\n",
      "Most similar to 'between':\n",
      "  betweens: 0.9137\n",
      "  betweenness: 0.9098\n",
      "  in-between: 0.8618\n",
      "  go-between: 0.8383\n",
      "  go-betweens: 0.8227\n",
      "  near-simultaneous: 0.7972\n",
      "  botha: 0.7857\n",
      "  arounds: 0.7829\n",
      "  paralleling: 0.7822\n",
      "  near-identical: 0.7817\n",
      "\n",
      "Most similar to 'the':\n",
      "  betweens: 0.9137\n",
      "  betweenness: 0.9098\n",
      "  in-between: 0.8618\n",
      "  go-between: 0.8383\n",
      "  go-betweens: 0.8227\n",
      "  near-simultaneous: 0.7972\n",
      "  botha: 0.7857\n",
      "  arounds: 0.7829\n",
      "  paralleling: 0.7822\n",
      "  near-identical: 0.7817\n",
      "\n",
      "Most similar to 'the':\n",
      "  thế: 1.0000\n",
      "  thé: 1.0000\n",
      "  thes: 0.9474\n",
      "  theary: 0.9317\n",
      "  theist: 0.9289\n",
      "  thet: 0.9286\n",
      "  thea: 0.9268\n",
      "  thel: 0.9267\n",
      "  thean: 0.9259\n",
      "  ther: 0.9241\n",
      "  thế: 1.0000\n",
      "  thé: 1.0000\n",
      "  thes: 0.9474\n",
      "  theary: 0.9317\n",
      "  theist: 0.9289\n",
      "  thet: 0.9286\n",
      "  thea: 0.9268\n",
      "  thel: 0.9267\n",
      "  thean: 0.9259\n",
      "  ther: 0.9241\n"
     ]
    }
   ],
   "source": [
    "# Function to find most similar words\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def most_similar_word(target_word, topn=10):\n",
    "    if target_word not in word_embeddings:\n",
    "        print(f\"'{target_word}' not in vocabulary\")\n",
    "        return []\n",
    "    \n",
    "    target_vec = word_embeddings[target_word]\n",
    "    similarities = []\n",
    "    \n",
    "    for word, embedding in word_embeddings.items():\n",
    "        if word != target_word:\n",
    "            sim = cosine_similarity(target_vec, embedding)\n",
    "            similarities.append((word, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:topn]\n",
    "\n",
    "# Test with various words\n",
    "test_words = ['cactus', 'cake', 'angry', 'quickly', 'between', 'the']\n",
    "\n",
    "for test_word in test_words:\n",
    "    print(f\"\\nMost similar to '{test_word}':\")\n",
    "    results = most_similar_word(test_word, topn=10)\n",
    "    for word, sim in results:\n",
    "        print(f\"  {word}: {sim:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc396-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
